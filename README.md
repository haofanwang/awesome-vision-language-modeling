# awesome-vision-language-pretraining [![Awesome](https://awesome.re/badge.svg)](https://awesome.re)
An up-to-date list of vision-language (VL) pre-training paper! Maintained by [Haofan Wang](https://haofanwang.github.io/) (haofanwang.ai@gmail.com).


## Pre-training
[SLIP: Self-supervision meets Language-Image Pre-training](https://arxiv.org/abs/2112.12750), [[code](https://github.com/facebookresearch/SLIP)], Meta AI.

[FILIP: Fine-grained Interactive Language-Image Pre-Training](https://arxiv.org/pdf/2111.07783.pdf), [[Unofficial code](https://github.com/lucidrains/x-clip)], Huawei.

[LiT: Zero-Shot Transfer with Locked-image Text Tuning](https://arxiv.org/abs/2111.07991), [[Unofficial code](https://github.com/lucidrains/x-clip)], Google.

[Prompting Visual-Language Models for Efficient Video Understanding](https://arxiv.org/pdf/2112.04478.pdf), [[code](https://github.com/ju-chen/Efficient-Prompt)], SJTU.

[TiP-Adapter: Training-free CLIP-Adapter for Better Vision-Language Modeling](https://arxiv.org/abs/2111.03930), [[code](https://github.com/gaopengcuhk/Tip-Adapter)], CUHK.


## Masked Image Modeling

[Masked Feature Prediction for Self-Supervised Visual Pre-Training](https://arxiv.org/abs/2112.09133), Meta AI.

[SimMIM: A Simple Framework for Masked Image Modeling](https://arxiv.org/abs/2111.09886), [[code](https://github.com/microsoft/SimMIM)], MSRA.

[PeCo: Perceptual Codebook for BERT Pre-training of Vision Transformers](https://arxiv.org/abs/2111.12710), [[code](https://github.com/microsoft/PeCo)], MSRA.

[Masked Autoencoders Are Scalable Vision Learners](https://arxiv.org/pdf/2111.06377.pdf), [[Unofficial code](https://github.com/pengzhiliang/MAE-pytorch)], Meta AI.

[iBOT: Image BERT Pre-Training with Online Tokenizer](https://arxiv.org/abs/2111.07832), [[code](https://github.com/bytedance/ibot)], ByteDance .

[BEiT: BERT Pre-Training of Image Transformers](https://arxiv.org/abs/2106.08254), [[code](https://github.com/microsoft/unilm)], MSRA.

## Masked Language Modeling
[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805), [[code](https://github.com/google-research/bert)], Google.
